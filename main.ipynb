{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from neuro.nn import activation, layer, losses, models, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "x_train = tf.transpose(x_train / 255.0, perm=[0, 3, 1, 2])\n",
    "x_test = tf.transpose(x_test / 255.0, perm=[0, 3, 1, 2])\n",
    "\n",
    "y_train = tf.one_hot(y_train[..., 0], 10)\n",
    "y_test = y_test[...,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential(\n",
    "    layer.Conv2D(3, 64, (5, 5), padding=2, stride=2),\n",
    "    activation.ReLU(),\n",
    "    layer.Conv2D(64, 32, (3, 3), padding=1),\n",
    "    activation.ReLU(),\n",
    "    layer.MaxPool2D((2, 2)),\n",
    "    layer.Flatten(),\n",
    "    layer.Dense(2048, 128),\n",
    "    activation.ReLU(),\n",
    "    layer.Dense(128, 10),\n",
    "    activation.StableSoftmax(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = losses.CategoricalCrossentropy()\n",
    "optim = optimizer.Adam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Training\")\n",
    "epochs = 1\n",
    "batch_size = 128\n",
    "for i in range(epochs):\n",
    "    start, end = 0, batch_size\n",
    "    batch_num = 0\n",
    "    while end < tf.shape(x_train)[0]:\n",
    "        batch_x, batch_y = x_train[start:end], y_train[start:end]\n",
    "        start, end = end, end + batch_size\n",
    "        # Forward Propagation\n",
    "        y_pred = model(batch_x)\n",
    "\n",
    "        # Calculation of Loss\n",
    "        train_loss = loss(y_pred, batch_y)\n",
    "        print(\n",
    "            f\"Epoch: {i + 1}, Batch: {batch_num}, Loss: {train_loss.numpy()}\")\n",
    "        batch_num += 1\n",
    "\n",
    "        # Back Propagation + Optimizing\n",
    "        optim(model, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = False\n",
    "predictions = model(x_test[:512])\n",
    "\n",
    "predictions = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "acc = sum(predictions.numpy() == y_test[:512]) / len(y_test[:512])\n",
    "print(f\"Test Accuracy: {acc * 100}%\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13eb0af71620a846e484681143862ad0a4deab6cadfc353264e5c14897c11035"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
